{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOstppxxThJKjFat+LxGRRl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abishek0070/Fine_Tuned_LLMs/blob/main/LLaMA-3-8B_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lrMJEXDKmL3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install bitsandbytes accelerate peft trl triton\n",
        "!pip install --upgrade xformers==0.0.29.post3 # reinstall xformers with upgrade to ensure dependencies\n",
        "!pip install cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"BNB_CUDA_BF16\"] = \"0\"\n",
        "\n",
        "import torch"
      ],
      "metadata": {
        "id": "dxJAqmg7g2EJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "model,tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = 2048,\n",
        "    dtype = torch.float16,\n",
        "    load_in_4bit = True,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "lom7ievHKoXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 4,\n",
        "    lora_alpha = 8,\n",
        "    lora_dropout = 0.1,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        ")\n"
      ],
      "metadata": {
        "id": "RdvmRohCMEfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=\"/content/linux_programming_dataset_500.json\",\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "print(type(dataset))  # sanity check\n",
        "\n",
        "def format_sample(example):\n",
        "    instruction = example[\"instruction\"]\n",
        "    input_text = example.get(\"input\", \"\")\n",
        "\n",
        "    if input_text.strip():\n",
        "        instruction = f\"{instruction}\\n\\nInput:\\n{input_text}\"\n",
        "\n",
        "    return {\n",
        "        \"text\": f\"\"\"### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\n",
        "{example['output']}\"\"\"\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kmfYHbWqObjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt(example):\n",
        "    prompt = f\"\"\"### Instruction:\n",
        "{example['instruction']}\n",
        "\n",
        "### Response:\n",
        "{example['output']}\"\"\"\n",
        "    return {\"text\": prompt}\n",
        "\n",
        "dataset = dataset.map(format_prompt)\n"
      ],
      "metadata": {
        "id": "eBIJpdf_OcSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "import torch\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 2048,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        num_train_epochs = 1,\n",
        "        learning_rate = 1e-4,\n",
        "        bf16 = False,\n",
        "        fp16 = True, # Re-enabling fp16 for T4 GPUs, as autocast conflict has been removed\n",
        "        logging_steps = 5,\n",
        "        output_dir = \"outputs\",\n",
        "        save_strategy = \"no\",\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "tscxqIMOiHmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "prompt = \"### Instruction:\\nHow would you debug a high CPU usage issue on a Linux server?\\n\\n### Response:\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens = 180,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.3,\n",
        "    do_sample = True,\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "qANkNJJnOpeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora_adapter\")\n",
        "tokenizer.save_pretrained(\"lora_adapter\")\n"
      ],
      "metadata": {
        "id": "oR59CRH8wxgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n"
      ],
      "metadata": {
        "id": "43kqHLA0xCWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "api.create_repo(\n",
        "    repo_id=\"Master-Abi/llama3-8b-linux-assistant-lora\",\n",
        "    private=False,\n",
        "    exist_ok=True,\n",
        ")\n",
        "\n",
        "api.upload_folder(\n",
        "    folder_path=\"lora_adapter\",\n",
        "    repo_id=\"Master-Abi/llama3-8b-linux-assistant-lora\",\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "6b3uMJ26xHJk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}